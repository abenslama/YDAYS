{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ceci est un guide étape par étape pour gratter les tweets Twitter à l'aide d'une bibliothèque Python appelée Tweepy.\n",
    "### Étude de cas: Covid 2019 vaccin 2019\n",
    "\n",
    "\n",
    "Prérequis: créer un compte de développeur Twitter\n",
    "Avant de commencer à utiliser Tweepy, vous aurez besoin d'un compte de développeur Twitter pour appeler les API de Twitter. \n",
    "\n",
    "Grattage par lots\n",
    "En raison du nombre limité d'appels d'API que l'on peut effectuer à l'aide d'un compte développeur basique et gratuit, (~ 900 appels toutes les 15 minutes avant que votre accès ne soit refusé), j'ai créé une fonction qui extrait 2500 tweets par exécution une fois toutes les 15 minutes\n",
    "\n",
    "\n",
    "En raison du nombre limité d'appels d'API que l'on peut effectuer à l'aide d'un compte développeur basique et gratuit, (~ 900 appels toutes les 15 minutes avant que votre accès ne soit refusé), j'ai créé une fonction qui extrait 2500 tweets par exécution une fois toutes les 15 minutes extrait 3,00 et plus, mais cela m'a été refusé après le deuxième lot). Dans cette fonction, vous spécifiez:\n",
    "paramètre de recherche tel que les mots clés et les hashtags, etc.\n",
    "date de début, après laquelle tous les tweets seraient extraits (vous ne pouvez extraire que les tweets qui ne sont pas plus anciens que les 7 derniers jours)\n",
    "nombre de tweets à tirer par exécution\n",
    "nombre d'exécutions toutes les 15 minutes\n",
    "J'ai extrait uniquement les métadonnées que j'ai jugées pertinentes pour mon cas. Vous pouvez explorer la liste des métadonnées de l'objet tweepy.Cursor en détail (c'est la vraie partie désordonnée).\n",
    "\n",
    "### On obtient les informations suivantes :\n",
    "\n",
    "        # user.screen_name - identifiant Twitter\n",
    "        # user.description - description du compte\n",
    "        # user.location - d'où vient-il tweeter\n",
    "        # user.friends_count - non. des autres utilisateurs que l'utilisateur suit (suivant)\n",
    "        # user.followers_count - non. des autres utilisateurs qui suivent cet utilisateur (suiveurs)\n",
    "        # user.statuses_count - total des tweets par utilisateur\n",
    "        # user.created_at - lors de la création du compte utilisateur\n",
    "        # created_at - quand le tweet a été créé\n",
    "        # retweet_count - non. de retweets\n",
    "        # (obsolète) user.favourites_count - probablement aucun total. de tweets favoris par l'utilisateur\n",
    "        # retweeted_status.full_text - texte intégral du tweet\n",
    "        # tweet.entities ['hashtags'] - hashtags dans le tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clés de votre application\n",
    "consumer_key = \"dO73ZQhKgX2mA9qAER6cVmVDT\"\n",
    "consumer_secret = \"YVjmIYiXpqRpRupVGCP4p6QzbA08lU4ihnhombPsn89KEM8rh4\"\n",
    "\n",
    "# le access_token est le token de l'application twitter que nous avons créée précédement\n",
    "access_token = \"1187541216-lgbnWetMfPJNP8BgiDOiBGwRNJCvLBGcuHfuurg\"\n",
    "access_token_secret = \"Y2r7pWLWMgvQgSh0EmdD0Imts4lPMLvQYq1SDYvFZuJ2H\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automating Scraping\n",
    "# Calls API every 15 minutes to prevent overcalling\n",
    "\n",
    "# 1. define a for-loop\n",
    "# 2. define search parameter\n",
    "# 3. define date period\n",
    "# 4. define no. of tweets to pull\n",
    "\n",
    "def collecttweets(search_words, date_since, numTweets, numRuns,language):\n",
    "\n",
    "    ## Arguments:\n",
    "    # search_words -> define a string of keywords for this function to extract\n",
    "    # date_since -> define a date from which to start extracting the tweets \n",
    "    # numTweets -> number of tweets to extract per run\n",
    "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
    "    ##\n",
    "    \n",
    "    # dataframe pandas pour stocker\n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        start_run2=time.asctime( time.localtime(time.time()) )\n",
    "        \n",
    "        \n",
    "        # Collectez les tweets à l'aide de l'objet Cursor \n",
    "        # .Cursor () renvoie un objet que vous pouvez parcourir ou boucler pour accéder aux données collectées. \n",
    "        # Chaque élément de l'itérateur a divers attributs auxquels vous pouvez accéder pour obtenir des informations sur chaque tweet\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=language, since=date_since, tweet_mode='extended').items(numTweets)\n",
    "\n",
    "        # Stockez ces tweets dans une liste python \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Obtenez les informations suivantes (méthodes pour les appeler): \n",
    "        # user.screen_name - pseudo twitter \n",
    "        # user.description - description du compte \n",
    "        # user.location - où tweete-t-il depuis \n",
    "        # user.friends_count - no. des autres utilisateurs que l'utilisateur suit (suivant) \n",
    "        # user.followers_count - no. des autres utilisateurs qui suivent cet utilisateur (followers) \n",
    "        # user.statuses_count - total des tweets par l'utilisateur \n",
    "        # user.created_at - lorsque le compte utilisateur a été créé \n",
    "        # created_at - lorsque le tweet a été créé \n",
    "        # retweet_count - no. de retweets \n",
    "        # (obsolète) user.favourites_count - probablement aucun total. de tweets favoris par l'utilisateur\n",
    "        # retweeted_status.full_text - texte intégral du tweet \n",
    "        # tweet.entities ['hashtags'] - hashtags dans le tweet\n",
    "        \n",
    "        # Commencez à gratter les tweets individuellement: \n",
    "\n",
    "        # Obtain the following info (methods to call them out):\n",
    "            # user.screen_name - twitter handle\n",
    "            # user.description - description of account\n",
    "            # user.location - where is he tweeting from\n",
    "            # user.friends_count - no. of other users that user is following (following)\n",
    "            # user.followers_count - no. of other users who are following this user (followers)\n",
    "            # user.statuses_count - total tweets by user\n",
    "            # user.created_at - when the user account was created\n",
    "            # created_at - when the tweet was created\n",
    "            # retweet_count - no. of retweets\n",
    "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "            # retweeted_status.full_text - full text of the tweet\n",
    "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "\n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "\n",
    "        for tweet in tweet_list:\n",
    "\n",
    "            # Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        end_run2=time.asctime( time.localtime(time.time()) )\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
    "        print('no. of tweets scraped for run {} start {}'.format(i, start_run2))\n",
    "        print('no. of tweets scraped for run {} end {}'.format(i, end_run2))                                                      \n",
    "        print('time take for {} run to complete is {} minutes.'.format(i, duration_run))\n",
    "        \n",
    "        \n",
    "        time.sleep(900) #15 minute sleep time\n",
    "\n",
    "        \n",
    "    # Once all runs have completed, save them to a single csv file:    \n",
    "    # Obtain timestamp in a readable format:\n",
    "    from datetime import datetime\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/' + to_csv_timestamp +'_'+'covid_tweets.csv'\n",
    "\n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    print('Scraping has completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 0 is 1500\n",
      "no. of tweets scraped for run 0 start Wed Jan 27 16:14:40 2021\n",
      "no. of tweets scraped for run 0 end Wed Jan 27 16:15:47 2021\n",
      "time take for 0 run to complete is 1.11 minutes.\n",
      "no. of tweets scraped for run 1 is 1500\n",
      "no. of tweets scraped for run 1 start Wed Jan 27 16:30:47 2021\n",
      "no. of tweets scraped for run 1 end Wed Jan 27 16:31:47 2021\n",
      "time take for 1 run to complete is 1.0 minutes.\n",
      "Scraping has completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialise these variables: \n",
    "search_words = \"#Covid_19 OR #vaccincovid OR #vaccination OR #vaccincovid\"\n",
    "date_since = \"2021-01-01\"\n",
    "numTweets = 1500\n",
    "numRuns = 2 \n",
    "language=\"fr\"\n",
    "# Call the function scraptweets\n",
    "collecttweets(search_words, date_since, numTweets, numRuns,language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
